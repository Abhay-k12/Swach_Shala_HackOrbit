{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6765,
     "status": "ok",
     "timestamp": 1751873494564,
     "user": {
      "displayName": "Abhay kanojia",
      "userId": "07946561113870187944"
     },
     "user_tz": -330
    },
    "id": "GNc7s8UOl69a",
    "outputId": "895b82d5-0585-49c1-f6e0-bfc0cf598a75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# libraries that we're loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, applications, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, save_img\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ci6Wx0dYtac3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32727,
     "status": "ok",
     "timestamp": 1751873527295,
     "user": {
      "displayName": "Abhay kanojia",
      "userId": "07946561113870187944"
     },
     "user_tz": -330
    },
    "id": "YWxJR_1ttcgs",
    "outputId": "8841ca0c-0e4a-4447-926f-bfcdf8dd1185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image augmentation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dir = '/content/drive/MyDrive/HackOrbit/Original_Images'\n",
    "output_dir = '/content/drive/MyDrive/HackOrbit/Original_Images_augmented'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "num_augmented_images = 30\n",
    "\n",
    "for category in os.listdir(input_dir):\n",
    "    category_path = os.path.join(input_dir, category)\n",
    "    if not os.path.isdir(category_path):\n",
    "        continue\n",
    "\n",
    "    save_category_path = os.path.join(output_dir, category)\n",
    "    if not os.path.exists(save_category_path):\n",
    "        os.makedirs(save_category_path)\n",
    "\n",
    "    # Loop through each image\n",
    "    for image_name in os.listdir(category_path):\n",
    "        image_path = os.path.join(category_path, image_name)\n",
    "        img = load_img(image_path)\n",
    "        x = img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "\n",
    "        # Generate augmented images\n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size=1,\n",
    "                                  save_to_dir=save_category_path,\n",
    "                                  save_prefix='aug',\n",
    "                                  save_format='jpeg'):\n",
    "            i += 1\n",
    "            if i >= num_augmented_images:\n",
    "                break\n",
    "\n",
    "print(\"Image augmentation completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1751873527299,
     "user": {
      "displayName": "Abhay kanojia",
      "userId": "07946561113870187944"
     },
     "user_tz": -330
    },
    "id": "HZWmnALHmZk7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loading the data and processing it so that we can transfer it into the model\n",
    "root_dir = \"/content/drive/MyDrive/HackOrbit/Original_Images_augmented\"\n",
    "classification_names = [\"DirtyFloor\", \"OverflowingDustbins\", \"TrashPresence\", \"WaterLeaks\"]\n",
    "\n",
    "# Function to convert the images into vectorize form and provide them label\n",
    "def load_images_and_labels(root_dir, classification_names, img_size=(224,224)):\n",
    "    # Creating an empty list to store the images and their corresponding labels\n",
    "    data = []\n",
    "\n",
    "    # Loading the images from each folder present into the brain folder\n",
    "    for i, classification_name in enumerate(classification_names):\n",
    "        # full path to the disease folder\n",
    "        classification_path = os.path.join(root_dir, classification_name)\n",
    "\n",
    "        # loading image files in the current disease folder\n",
    "        image_files = os.listdir(classification_path)\n",
    "\n",
    "        # getting each image file in the current folder\n",
    "        for image_file in image_files:\n",
    "\n",
    "            # Checking image\n",
    "            if image_file.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "                # full path to the image file\n",
    "                image_path = os.path.join(classification_path, image_file)\n",
    "\n",
    "                # Loading the image using OpenCV\n",
    "                img = cv2.imread(image_path)\n",
    "                if img is None:\n",
    "                    print(f\"Warning: Could not read image: {image_path}\")\n",
    "                    continue    # Skip to the next image if loading fails\n",
    "\n",
    "                # resizing the image to a consistent size and converting into the grayscale image\n",
    "                img = cv2.resize(img, img_size)\n",
    "                label = i\n",
    "                data.append((img, label))\n",
    "\n",
    "    print(f\"Total images loaded: {len(data)}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to apply remove_noise and split the initial corpus into vectors of image and labels\n",
    "def preprocess_data(data):\n",
    "    corpus = []\n",
    "    labels = []\n",
    "    for img, label in data:\n",
    "        # appending the denoised image to the new corpus\n",
    "        corpus.append(img)\n",
    "        labels.append(label)\n",
    "    return np.array(corpus), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare the dataset for training the model\n",
    "def prepare_dataset(images, labels, batch_size=32, shuffle=True):\n",
    "    # normalizing the images and expand dims for channel\n",
    "    images = images.astype('float32') / 255.0\n",
    "    images = np.expand_dims(images, axis=-1)\n",
    "    labels = to_categorical(labels, num_classes=len(classification_names))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cnn model to process the images\n",
    "class ClassifierOptimized:\n",
    "    # constructor to initalise the values\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    # function to buil the model\n",
    "    def build_cnn(self):\n",
    "        inputs = layers.Input(shape=self.input_shape)\n",
    "        x = layers.Conv2D(32, (7, 7), padding='same')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "        # adjust the residual connection to match the number of filters\n",
    "        residual = layers.Conv2D(64, (1, 1), padding='same')(x)  # Matching the filter size\n",
    "        residual = layers.BatchNormalization()(residual)\n",
    "\n",
    "        x = layers.Conv2D(64, (3, 3), padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.add([x, residual])  # Adding the residual connection\n",
    "\n",
    "        se = layers.GlobalAveragePooling2D()(x)\n",
    "        se = layers.Dense(64 // 16, activation='relu')(se)\n",
    "        se = layers.Dense(64, activation='sigmoid')(se)\n",
    "        se = layers.Reshape((1, 1, 64))(se)\n",
    "        x = layers.multiply([x, se])\n",
    "\n",
    "        x = layers.Conv2D(128, (3, 3), padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "\n",
    "        # compliling the model together with the inputs and outputs layers\n",
    "        model = models.Model(inputs, outputs)\n",
    "        model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')],\n",
    "        )\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMx+BwaBa24UrdbTLShuYWc",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
